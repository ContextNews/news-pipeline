name: Run Pipeline

on:
  schedule:
    - cron: "0 6,18 * * *"
  workflow_dispatch:
    inputs:
      # Ingest inputs
      ingest-lookback-hours:
        description: 'Number of hours to look back for articles'
        type: number
        default: 12
      ingest-sources:
        description: 'Sources'
        type: string
        default: 'all'
      # Embed inputs
      embed-published-date:
        description: 'Embed articles published on this date (UTC, YYYY-MM-DD)'
        type: string
        default: ''
      embed-model:
        description: 'Sentence-transformers model name'
        type: string
        default: 'all-MiniLM-L6-v2'
      embed-batch-size:
        description: 'Batch size for encoding'
        type: number
        default: 32
      embed-word-limit:
        description: 'Embedded words limit'
        type: number
        default: 300
      embed-no-title:
        description: 'Exclude title from embedding'
        type: boolean
        default: false
      embed-no-summary:
        description: 'Exclude summary from embedding'
        type: boolean
        default: false
      embed-no-text:
        description: 'Exclude text from embedding'
        type: boolean
        default: false
      embed-overwrite:
        description: 'Re-embed articles even if already embedded for this model'
        type: boolean
        default: false
      # Extract entities inputs
      entities-published-date:
        description: 'Extract entities from articles published on this date (UTC, YYYY-MM-DD)'
        type: string
        default: ''
      entities-model:
        description: 'spaCy model to use'
        type: string
        default: 'en_core_web_trf'
      entities-batch-size:
        description: 'Batch size for spaCy NER'
        type: number
        default: 32
      entities-word-limit:
        description: 'Entity extraction word limit'
        type: number
        default: 300
      entities-overwrite:
        description: 'Re-extract entities for articles with existing entities'
        type: boolean
        default: false
      # Cluster inputs
      cluster-ingested-date:
        description: 'UTC date to load from RDS (YYYY-MM-DD)'
        type: string
        default: ''
      cluster-embedding-model:
        description: 'Embedding model to use'
        type: string
        default: 'all-MiniLM-L6-v2'
      cluster-min-cluster-size:
        description: 'Minimum cluster size'
        type: number
        default: 2
      cluster-min-samples:
        description: 'Minimum samples (leave empty for default)'
        type: number
        default: 0
      cluster-overwrite-clusters:
        description: 'Overwrite existing clusters'
        type: boolean
        default: true
      # Generate stories inputs
      stories-cluster-period:
        description: 'UTC cluster period to process (YYYY-MM-DD)'
        type: string
        default: ''
      stories-model:
        description: 'OpenAI model to use'
        type: string
        default: 'gpt-4o-mini'
      stories-overwrite-stories:
        description: 'Overwrite existing stories'
        type: boolean
        default: true

permissions:
  id-token: write
  contents: read

jobs:
  ingest:
    uses: ./.github/workflows/ingest_articles.yaml
    secrets: inherit
    with:
      lookback-hours: ${{ fromJSON(inputs.ingest-lookback-hours || '12') }}
      sources: ${{ inputs.ingest-sources || 'all' }}
      load-s3: true
      load-rds: true

  # Embed and extract-entities run in parallel after ingest
  embed:
    needs: ingest
    uses: ./.github/workflows/compute_embeddings.yaml
    secrets: inherit
    with:
      published-date: ${{ inputs.embed-published-date || '' }}
      model: ${{ inputs.embed-model || 'all-MiniLM-L6-v2' }}
      batch-size: ${{ fromJSON(inputs.embed-batch-size || '32') }}
      word-limit: ${{ fromJSON(inputs.embed-word-limit || '300') }}
      no-title: ${{ inputs.embed-no-title == true }}
      no-summary: ${{ inputs.embed-no-summary == true }}
      no-text: ${{ inputs.embed-no-text == true }}
      overwrite: ${{ inputs.embed-overwrite == true }}
      load-s3: true
      load-rds: true

  extract-entities:
    needs: ingest
    uses: ./.github/workflows/extract_entities.yaml
    secrets: inherit
    with:
      published-date: ${{ inputs.entities-published-date || '' }}
      model: ${{ inputs.entities-model || 'en_core_web_trf' }}
      batch-size: ${{ fromJSON(inputs.entities-batch-size || '32') }}
      word-limit: ${{ fromJSON(inputs.entities-word-limit || '300') }}
      overwrite: ${{ inputs.entities-overwrite == true }}
      load-s3: true
      load-rds: true

  # Cluster waits for both embed and extract-entities
  cluster:
    needs: [embed, extract-entities]
    uses: ./.github/workflows/cluster_articles.yaml
    secrets: inherit
    with:
      ingested-date: ${{ inputs.cluster-ingested-date || '' }}
      embedding-model: ${{ inputs.cluster-embedding-model || 'all-MiniLM-L6-v2' }}
      min-cluster-size: ${{ fromJSON(inputs.cluster-min-cluster-size || '2') }}
      min-samples: ${{ fromJSON(inputs.cluster-min-samples || '0') }}
      load-s3: true
      load-rds: true
      overwrite-clusters: ${{ inputs.cluster-overwrite-clusters != false }}

  # Generate stories after clustering
  generate-stories:
    needs: cluster
    uses: ./.github/workflows/generate_stories.yaml
    secrets: inherit
    with:
      cluster-period: ${{ inputs.stories-cluster-period || '' }}
      model: ${{ inputs.stories-model || 'gpt-4o-mini' }}
      load-s3: true
      load-rds: true
      overwrite-stories: ${{ inputs.stories-overwrite-stories != false }}
