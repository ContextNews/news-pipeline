name: Cluster Articles

on:
  workflow_call:
    inputs:
      ingested-date:
        type: string
        default: ''
      embedding-model:
        type: string
        default: 'all-MiniLM-L6-v2'
      min-cluster-size:
        type: number
        default: 2
      min-samples:
        type: number
        default: 0
      load-s3:
        type: boolean
        default: true
      load-rds:
        type: boolean
        default: true
      overwrite-clusters:
        type: boolean
        default: true
  workflow_dispatch:
    inputs:
      ingested-date:
        description: 'UTC date to load from RDS (YYYY-MM-DD)'
        type: string
        required: false
        default: ''
      embedding-model:
        description: 'Embedding model to use'
        type: string
        default: 'all-MiniLM-L6-v2'
      min-cluster-size:
        description: 'Minimum cluster size'
        type: number
        default: 2
      min-samples:
        description: 'Minimum samples (leave empty for default)'
        type: number
        default: 0
      load-s3:
        description: 'Load to S3'
        type: boolean
        default: true
      load-rds:
        description: 'Load to RDS'
        type: boolean
        default: true
      overwrite-clusters:
        description: 'Overwrite existing clusters'
        type: boolean
        default: true

jobs:
  cluster-articles:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: eu-west-2

      - name: Set up SSH tunnel
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.BASTION_SSH_KEY }}" > ~/.ssh/bastion_key
          chmod 600 ~/.ssh/bastion_key

          ssh-keyscan -H ${{ secrets.BASTION_IP }} >> ~/.ssh/known_hosts 2>&1

          ssh -f -N -L 5432:${{ secrets.RDS_ENDPOINT }}:5432 \
            -o ExitOnForwardFailure=yes \
            -o ConnectTimeout=10 \
            -o ServerAliveInterval=60 \
            -i ~/.ssh/bastion_key \
            ec2-user@${{ secrets.BASTION_IP }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: latest
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-cluster-articles-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        run: poetry install --no-interaction --with cluster

      - name: Build command arguments
        id: build-args
        run: |
          if [ -n "${{ inputs.ingested-date }}" ]; then
            ARGS="--ingested-date ${{ inputs.ingested-date }}"
          else
            ARGS="--ingested-date $(date -u +%F)"
          fi
          ARGS="$ARGS --embedding-model ${{ inputs.embedding-model }}"
          ARGS="$ARGS --min-cluster-size ${{ inputs.min-cluster-size }}"
          if [ "${{ inputs.min-samples }}" -gt 0 ]; then
            ARGS="$ARGS --min-samples ${{ inputs.min-samples }}"
          fi
          if [ "${{ inputs.load-s3 }}" = "true" ]; then
            ARGS="$ARGS --load-s3"
          fi
          if [ "${{ inputs.load-rds }}" = "true" ]; then
            ARGS="$ARGS --load-rds"
          fi
          if [ "${{ inputs.overwrite-clusters }}" = "false" ]; then
            ARGS="$ARGS --no-overwrite-clusters"
          fi
          echo "args=$ARGS" >> $GITHUB_OUTPUT

      - name: Run cluster_articles
        env:
          PYTHONPATH: src
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
          DATABASE_URL: postgresql://postgres:${{ secrets.DB_PASSWORD }}@localhost:5432/contextdb
        run: poetry run python -m cluster_articles ${{ steps.build-args.outputs.args }}
